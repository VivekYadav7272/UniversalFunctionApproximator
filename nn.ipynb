{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryNeuralNetwork:\n",
    "    def __init__(self, input_size: int, hidden_layers: int, nodes_in_each_layer: int):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.nodes_in_hidden_layer = nodes_in_each_layer\n",
    "\n",
    "        self.weights = [np.random.randn(input_size, nodes_in_each_layer) / np.sqrt(input_size)]\n",
    "        self.bias = [np.random.randn(nodes_in_each_layer, 1)]\n",
    "\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            self.weights.append(np.random.randn(nodes_in_each_layer, nodes_in_each_layer) / np.sqrt(input_size))\n",
    "            self.bias.append(np.random.randn(nodes_in_each_layer, 1))\n",
    "        \n",
    "        self.weights.append(np.random.randn(nodes_in_each_layer, 1) / np.sqrt(input_size))\n",
    "        self.bias.append(np.random.randn(1, 1))\n",
    "\n",
    "    \n",
    "    def forward_propagate(self, input_xs: np.ndarray):\n",
    "        self.num_examples = input_xs.shape[1]\n",
    "        self.activations = [input_xs]\n",
    "        self.Z = []\n",
    "        for i in range(0, self.hidden_layers + 1):\n",
    "            Z: np.ndarray = self.weights[i].T.dot(self.activations[i]) + self.bias[i]\n",
    "            if i != self.hidden_layers:\n",
    "                assert Z.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "            else:\n",
    "                assert Z.shape == (1, self.num_examples)\n",
    "\n",
    "            activation = leaky_relu if i != self.hidden_layers else sigmoid\n",
    "            self.activations.append(activation(Z))\n",
    "            self.Z.append(Z)\n",
    "\n",
    "        self.output = self.activations[-1]\n",
    "        assert self.output.shape == (1, self.num_examples)\n",
    "        # print(self.output)\n",
    "\n",
    "    \n",
    "    def backpropagate(self, inp_ys: np.ndarray, learning_rate: float):\n",
    "        (dL_dOutputW, dL_dOutputB, dL_dA_Lminus1) = self.output_loss(inp_ys)\n",
    "        (dWeights, dBiases) = self.hidden_loss(dL_dA_Lminus1)\n",
    "        # print(f\"dWeights = {dWeights}\\n\\ndBiases = {dBiases}\")\n",
    "\n",
    "        self.weights[-1] -= learning_rate * dL_dOutputW\n",
    "        self.bias[-1] -= learning_rate * dL_dOutputB\n",
    "\n",
    "\n",
    "        for i in range(self.hidden_layers):\n",
    "            # print(i)\n",
    "            self.weights[i] -= learning_rate * dWeights[i]\n",
    "            self.bias[i] -= learning_rate * dBiases[i]\n",
    "            \n",
    "    \n",
    "\n",
    "    def output_loss(self, inp_ys: np.ndarray) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "        # ------------------ For Output Layer, ---------------------------------------\n",
    "\n",
    "        # print(f\"inp_ys = {inp_ys.shape} && self.output == {self.output.shape}\")\n",
    "        assert inp_ys.shape == self.output.shape\n",
    "\n",
    "        dL_dOutputA = -(inp_ys - self.output)\n",
    "        assert dL_dOutputA.shape == (1, self.num_examples)\n",
    "        dA_dOutputZ = sigmoid_derivative(self.activations[-1])\n",
    "        assert dA_dOutputZ.shape == (1, self.num_examples)\n",
    "        \n",
    "        dL_dOutputZ = dL_dOutputA * dA_dOutputZ\n",
    "        assert dL_dOutputZ.shape == (1, self.num_examples)\n",
    "\n",
    "        dOutputZ_dW = self.activations[-2]\n",
    "        assert dOutputZ_dW.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "\n",
    "        # dL_dOutputZ should be broadcasted up for each of the weights\n",
    "        dL_dOutputW = dL_dOutputZ * dOutputZ_dW\n",
    "        assert dL_dOutputW.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "        dL_dOutputW = np.sum(dL_dOutputW, axis=1, keepdims=True) / self.num_examples\n",
    "        assert dL_dOutputW.shape == (self.nodes_in_hidden_layer, 1)\n",
    "\n",
    "        # print(f\"dL_dOutputW = {dL_dOutputW}\")\n",
    "\n",
    "        dL_dOutputB = dL_dOutputZ # (dOutuptZ_dB == 1)\n",
    "        dL_dOutputB = np.sum(dL_dOutputB, axis=1, keepdims=True) / self.num_examples\n",
    "        assert dL_dOutputB.shape == (1, 1)\n",
    "        # print(f\"dL_dOutputB = {dL_dOutputB}\")\n",
    "\n",
    "        # -------------------- Done For Output Layer ---------------------------------------\n",
    "        # Now those are corrections made for this layer, but to propagate error backwards,\n",
    "        # we need to calculate error for the previous nodes as well.\n",
    "\n",
    "        dL_dA_l_1 = dL_dOutputZ * self.weights[-1] # dOutputZ_dA_l_1\n",
    "        assert dL_dA_l_1.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "\n",
    "        return (dL_dOutputW, dL_dOutputB, dL_dA_l_1)\n",
    "    \n",
    "\n",
    "    def hidden_loss(self, dLoss_dA_Lminus1: np.ndarray) -> (list[np.ndarray], list[np.ndarray]):\n",
    "        # dLoss_dA_curr_layer is done for all self.num_examples.\n",
    "        dLoss_dA_curr_layer = dLoss_dA_Lminus1\n",
    "        # print(f\"dLoss_dA_Lminus1 = {dLoss_dA_curr_layer}\")\n",
    "        assert dLoss_dA_curr_layer.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "        \n",
    "        dLoss_dWs = []\n",
    "        dLoss_dBs = []\n",
    "\n",
    "        for i in range(self.hidden_layers - 1, -1, -1):\n",
    "            dA_dZ = leaky_relu_derivative(self.Z[i])\n",
    "            assert dA_dZ.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "            \n",
    "            dLoss_dZ = dLoss_dA_curr_layer * dA_dZ\n",
    "            assert dLoss_dZ.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "\n",
    "            dZ_dW = self.activations[i]\n",
    "            # Really, it should be a 3D array, but it would actually just be a broadcast of each column, putting the examples in the 3rd dimension.\n",
    "            if i != 0:\n",
    "                assert dZ_dW.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "            else:\n",
    "                assert dZ_dW.shape == (self.input_size, self.num_examples)\n",
    "\n",
    "            # What's happening: dLoss_dZ's outer product with dZ_dW and then averaged over all training examples.\n",
    "            # dZ_dW is actually supposed to be a 2D matrix for one example.\n",
    "            # dLoss_dZ is supposed to be a 1D vector for one example.\n",
    "            # dLoss_dZ = [dLoss_dZ1, dLoss_dZ2, ....]\n",
    "            # dZ_dW    =[[dZ_dW00,   dZ_dW01,   dZ_dW02, ..]\n",
    "            #            [dZ_dW10,   dZ_dW11,   dZ_dW12, ..]\n",
    "            #               ...\n",
    "            #           ]\n",
    "            # But if you think about it, it's actually just:\n",
    "            # dZ_dW    =[[A(L-2, 0), A(L-2, 0), A(L-2, 0), ..]\n",
    "            #            [A(L-2, 1), A(L-2, 1), A(L-2, 1), ..]\n",
    "            #            ...\n",
    "            #           ]\n",
    "            # So really, for one example, it's just a 1D vector broadcasted into 2D.                    VVVV 1D part             , 1D part for all examples\n",
    "            # Hence our dZ_dW is technically having all information needed since its dimensions are (self.num_nodes_in_each_layer, self.num_examples).\n",
    "            # So, what we really want dLoss_dW to be is outer product of dLoss_dZ (1D form) and dZ_dW (1D form).\n",
    "            # and then since we have training examples, we average it over all training examples.\n",
    "            # Finally, the resulting weight differential should be arranged as (prev_layer X current_layer),\n",
    "            # and the resulting weight differential out of dL/dZ @ dZ/dW is actually (current_layer X prev_layer), \n",
    "            # so instead we do dZ/dW @ dL/dZ.\n",
    "            # This is what the simple dot product expression is doing. Magic to me how it all worked out almost coincidentally so well.\n",
    "            dLoss_dW = (dZ_dW.dot(dLoss_dZ.T)) / self.num_examples\n",
    "            if i != 0:\n",
    "                assert dLoss_dW.shape == (self.nodes_in_hidden_layer, self.nodes_in_hidden_layer)\n",
    "            else:\n",
    "                assert dLoss_dW.shape == (self.input_size, self.nodes_in_hidden_layer)\n",
    "            \n",
    "            dLoss_dWs.append(dLoss_dW)\n",
    "\n",
    "            dLoss_dB = np.sum(dLoss_dZ, axis=1, keepdims=True) / self.num_examples\n",
    "            assert dLoss_dB.shape == (self.nodes_in_hidden_layer, 1)\n",
    "\n",
    "            dLoss_dBs.append(dLoss_dB)\n",
    "\n",
    "            dLoss_dA_curr_layer = self.weights[i].dot(dLoss_dZ)\n",
    "            \n",
    "            # print(f\"dLoss_dA_curr_layer = {dLoss_dA_curr_layer}\")\n",
    "            \n",
    "            if i != 0:\n",
    "                assert dLoss_dA_curr_layer.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "            else:\n",
    "                assert dLoss_dA_curr_layer.shape == (self.input_size, self.num_examples)\n",
    "\n",
    "        return (list(reversed(dLoss_dWs)), list(reversed(dLoss_dBs)))\n",
    "            \n",
    "\n",
    "def sigmoid(x):\n",
    "    # Check if any element in the matrix is > 100_000\n",
    "    if np.any(x > 100_000):\n",
    "        print(x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(g_x):\n",
    "    return g_x * (1 - g_x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x < 0, 0, 1)\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return np.maximum(0.01*x, x)\n",
    "\n",
    "def leaky_relu_derivative(x: np.ndarray):\n",
    "    return np.where(x < 0, 0.01, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43483/290922660.py:6: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  ys = np.array([float(int(x[1] % x[0] == 0)) for x in xs.T]).reshape(1, examples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 4 0 ... 7 1 6]\n",
      " [7 0 6 ... 5 0 9]] [[1. 1. 1. ... 0. 1. 0.]]\n",
      "Output = [[1.         0.99999987 1.         ... 0.99987295 0.99966877 0.99987295]]\n",
      "Expected = [[0. 0. 1. ... 1. 1. 1.]]\n",
      "Accuracy = 0.4237\n",
      "Number of one classes = 8474.0, total examples = 20000\n",
      "\n",
      "[array([[ 0.90930575, -0.1311623 , -0.20493618, -0.85045517, -0.97454823],\n",
      "       [ 1.29981292, -0.32923034, -0.40592994,  0.07651804,  1.36811296]]), array([[-0.67342477,  0.34660589,  1.21908679,  1.1399262 , -1.64668184],\n",
      "       [ 1.18298334,  0.05499343, -0.61664143, -0.01833464, -0.71349218],\n",
      "       [ 0.72363393, -0.12664726,  0.02639522,  1.98884081, -0.99404407],\n",
      "       [ 0.04003145,  0.47740134,  1.31017365,  0.1279123 , -1.33018454],\n",
      "       [-1.1148108 ,  1.22285917, -1.40125041, -1.30061228,  0.79779575]]), array([[-0.25951014, -0.3894772 ,  0.57745053, -0.47728579, -0.21301921],\n",
      "       [ 0.59761615,  0.61402464,  0.10752936, -0.78696983, -0.28819571],\n",
      "       [-0.16750222,  1.02889863,  0.01868759, -0.1891239 , -1.57779298],\n",
      "       [-0.26957839, -0.9829868 ,  0.87993527,  0.5659455 ,  0.39302292],\n",
      "       [-0.9030535 ,  0.95246781,  1.05954866, -0.69412412, -1.36435886]]), array([[ 0.02954949, -0.38071026, -0.34875061, -1.34798251, -0.19346335],\n",
      "       [-0.08153549, -0.38042143,  1.54884078, -1.14329372, -0.40163845],\n",
      "       [-0.64381744, -0.48519892, -0.0943695 ,  0.17291944,  0.78563958],\n",
      "       [ 0.7918392 ,  1.03223165, -0.28412352, -1.58007435, -0.51514026],\n",
      "       [ 1.90767954,  1.17254765,  0.56523345,  0.20892524, -0.00358421]]), array([[-0.44749645,  1.54718143,  0.43256929, -0.20902951,  1.13636202],\n",
      "       [-0.5194455 , -2.0658053 ,  1.87946421,  1.38595463, -1.56127302],\n",
      "       [ 0.14742098, -0.50861183, -0.92001835,  0.37890705, -1.51373692],\n",
      "       [-0.15569677, -1.94632245, -0.97905799, -0.70130338,  0.58752898],\n",
      "       [-0.10410562,  1.86395398, -0.20941863, -0.61628813,  1.91867217]]), array([[-0.00355196, -0.3335649 , -1.32467439,  0.83092453,  1.48983852],\n",
      "       [-0.54315487, -0.56192076,  1.49663372,  0.98569954, -0.2463997 ],\n",
      "       [-0.46887343,  1.11727924,  2.07472093, -0.19774255,  0.33574158],\n",
      "       [-0.36547442,  1.20564485, -0.80624344, -0.89499936, -1.175291  ],\n",
      "       [-0.12922732,  0.12781043, -1.27621439,  0.5752166 , -1.44618167]]), array([[-1.9589816 , -0.25962347, -0.86243167, -0.08182472, -0.17488625],\n",
      "       [ 0.1904622 ,  0.47876267,  1.91849605,  0.37610447,  1.09679602],\n",
      "       [-0.34861908,  0.32249061,  2.06595092,  0.79862793,  1.34256345],\n",
      "       [-1.14884143,  0.96576918,  1.34324779,  1.90802513,  0.46384178],\n",
      "       [ 0.63625529,  0.00702879,  0.69375569, -0.53891537,  0.87874219]]), array([[ 0.82220263, -0.06985119, -0.65647033,  0.10056372,  0.45011296],\n",
      "       [ 1.44147114, -1.96844874, -0.51650456, -0.76157225,  1.41435394],\n",
      "       [-0.64079712, -0.2457786 ,  0.47801921,  0.78612464, -0.0764537 ],\n",
      "       [ 0.82022228, -0.54729758, -1.08901896, -0.6537043 ,  0.44046834],\n",
      "       [-1.85069056,  1.33446984, -0.6968909 , -0.36940102, -0.97514914]]), array([[ 0.16979378,  0.77045872,  0.52262421, -0.71646369,  0.4971432 ],\n",
      "       [-0.13048355,  0.20292648,  0.51547355, -0.99471771, -0.33699555],\n",
      "       [-0.8662867 , -0.16425608, -1.32932952, -2.02528101, -0.22520405],\n",
      "       [ 0.30722984,  0.01365632, -1.60712765,  0.7496632 ,  1.26132927],\n",
      "       [-0.52637442,  1.38196789,  0.14337285,  1.08110468, -0.9630704 ]]), array([[ 1.19966495,  0.8368836 , -0.45507831, -0.16811607,  0.59293694],\n",
      "       [ 0.00570064, -0.00735079,  0.60493825, -0.20472434,  0.68936654],\n",
      "       [ 1.29504811,  0.25402325, -0.32322   , -0.15217359, -0.13375415],\n",
      "       [-0.34731597,  0.0439961 , -0.08197984, -0.20908631, -0.43520699],\n",
      "       [-2.30485666,  0.86382618, -0.99350324, -0.35376262,  1.88157062]]), array([[1.18266019],\n",
      "       [1.48754307],\n",
      "       [0.28629302],\n",
      "       [0.08418501],\n",
      "       [2.38877475]])]\n",
      "[array([[-0.4673864 ],\n",
      "       [-0.01529394],\n",
      "       [ 1.06262432],\n",
      "       [ 0.43595162],\n",
      "       [ 1.17386103]]), array([[-0.20256517],\n",
      "       [-0.83444105],\n",
      "       [-0.6590611 ],\n",
      "       [ 1.6019909 ],\n",
      "       [ 0.56794114]]), array([[-0.39224678],\n",
      "       [-0.81688762],\n",
      "       [-1.31691389],\n",
      "       [ 0.16575546],\n",
      "       [-2.73836573]]), array([[-0.13011602],\n",
      "       [ 0.00283204],\n",
      "       [ 1.13842945],\n",
      "       [ 1.26621775],\n",
      "       [-1.75369044]]), array([[ 0.45113719],\n",
      "       [ 0.20122889],\n",
      "       [ 0.99392306],\n",
      "       [-0.81105618],\n",
      "       [-0.32629646]]), array([[-1.13538478],\n",
      "       [-0.93081503],\n",
      "       [ 0.91945137],\n",
      "       [ 0.88002841],\n",
      "       [-1.75506954]]), array([[ 0.77420756],\n",
      "       [-0.12477727],\n",
      "       [-1.10986047],\n",
      "       [-0.37281234],\n",
      "       [-0.48995177]]), array([[ 2.13817714e+00],\n",
      "       [ 1.53028957e-02],\n",
      "       [ 1.99429529e-03],\n",
      "       [-4.88802543e-01],\n",
      "       [ 2.94078374e-01]]), array([[-0.94824584],\n",
      "       [-0.00651305],\n",
      "       [-0.67864723],\n",
      "       [-1.85596256],\n",
      "       [-0.33888344]]), array([[2.37061661],\n",
      "       [1.14124121],\n",
      "       [0.42874507],\n",
      "       [1.01228968],\n",
      "       [0.40191182]]), array([[1.68711997]])]\n"
     ]
    }
   ],
   "source": [
    "# Create a neural network to determine if first number is a factor of second or not.\n",
    "nn = BinaryNeuralNetwork(2, 10, 5)\n",
    "# Generate training data.\n",
    "examples = 100_000\n",
    "xs = np.random.randint(0., 10., (2, examples))\n",
    "ys = np.array([float(int(x[1] % x[0] == 0)) for x in xs.T]).reshape(1, examples)\n",
    "# Split into training and testing data as a fraction of examples\n",
    "train_xs = xs[:, :int(0.8 * examples)]\n",
    "train_ys = ys[:, :int(0.8 * examples)]\n",
    "print(train_xs, train_ys)\n",
    "test_xs = xs[:, int(0.8 * examples):]\n",
    "test_ys = ys[:, int(0.8 * examples):]\n",
    "# Train the neural network with 500 examples at a time\n",
    "for _ in range(150):\n",
    "    for i in range(0, train_xs.shape[1], 1000):\n",
    "        nn.forward_propagate(train_xs[:, i:i+1000])\n",
    "        nn.backpropagate(train_ys[:, i:i+1000], 0.01)\n",
    "\n",
    "\n",
    "    nn.forward_propagate(test_xs)\n",
    "    print(f\"Output = {nn.output}\")\n",
    "    print(f\"Expected = {test_ys}\")\n",
    "    print(f\"Accuracy = {np.sum(np.round(nn.output) == test_ys) / test_ys.shape[1]}\")\n",
    "    print(f\"Number of one classes = {np.sum(test_ys)}, total examples = {test_ys.shape[1]}\")\n",
    "    # Test the neural network\n",
    "\n",
    "\n",
    "print(nn.weights)\n",
    "print(nn.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

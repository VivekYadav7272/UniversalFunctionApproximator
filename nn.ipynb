{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryNeuralNetwork:\n",
    "    def __init__(self, input_size: int, hidden_layers: int, nodes_in_each_layer: int):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.nodes_in_hidden_layer = nodes_in_each_layer\n",
    "\n",
    "        self.weights = [np.random.randn(input_size, nodes_in_each_layer) / np.sqrt(input_size)]\n",
    "        self.bias = [np.random.randn(nodes_in_each_layer, 1)]\n",
    "\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            self.weights.append(np.random.randn(nodes_in_each_layer, nodes_in_each_layer) / np.sqrt(input_size))\n",
    "            self.bias.append(np.random.randn(nodes_in_each_layer, 1))\n",
    "        \n",
    "        self.weights.append(np.random.randn(nodes_in_each_layer, 1) / np.sqrt(input_size))\n",
    "        self.bias.append(np.random.randn(1, 1))\n",
    "\n",
    "    \n",
    "    def forward_propagate(self, input_xs: np.ndarray):\n",
    "        self.num_examples = input_xs.shape[1]\n",
    "        self.activations = [input_xs]\n",
    "        self.Z = []\n",
    "        for i in range(0, self.hidden_layers + 1):\n",
    "            Z: np.ndarray = self.weights[i].T.dot(self.activations[i]) + self.bias[i]\n",
    "            if i != self.hidden_layers:\n",
    "                assert Z.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "            else:\n",
    "                assert Z.shape == (1, self.num_examples)\n",
    "\n",
    "            activation = leaky_relu if i != self.hidden_layers else sigmoid\n",
    "            self.activations.append(activation(Z))\n",
    "            self.Z.append(Z)\n",
    "\n",
    "        self.output = self.activations[-1]\n",
    "        assert self.output.shape == (1, self.num_examples)\n",
    "        # print(self.output)\n",
    "\n",
    "    \n",
    "    def backpropagate(self, inp_ys: np.ndarray, learning_rate: float):\n",
    "        (dL_dOutputW, dL_dOutputB, dL_dA_Lminus1) = self.output_loss(inp_ys)\n",
    "        (dWeights, dBiases) = self.hidden_loss(dL_dA_Lminus1)\n",
    "        # print(f\"dWeights = {dWeights}\\n\\ndBiases = {dBiases}\")\n",
    "\n",
    "        self.weights[-1] -= learning_rate * dL_dOutputW\n",
    "        self.bias[-1] -= learning_rate * dL_dOutputB\n",
    "\n",
    "\n",
    "        for i in range(self.hidden_layers):\n",
    "            # print(i)\n",
    "            self.weights[i] -= learning_rate * dWeights[i]\n",
    "            self.bias[i] -= learning_rate * dBiases[i]\n",
    "            \n",
    "    \n",
    "\n",
    "    def output_loss(self, inp_ys: np.ndarray) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "        # ------------------ For Output Layer, ---------------------------------------\n",
    "\n",
    "        # print(f\"inp_ys = {inp_ys.shape} && self.output == {self.output.shape}\")\n",
    "        assert inp_ys.shape == self.output.shape\n",
    "\n",
    "        dL_dOutputA = -(inp_ys - self.output)\n",
    "        assert dL_dOutputA.shape == (1, self.num_examples)\n",
    "        dA_dOutputZ = sigmoid_derivative(self.activations[-1])\n",
    "        assert dA_dOutputZ.shape == (1, self.num_examples)\n",
    "        \n",
    "        dL_dOutputZ = dL_dOutputA * dA_dOutputZ\n",
    "        assert dL_dOutputZ.shape == (1, self.num_examples)\n",
    "\n",
    "        dOutputZ_dW = self.activations[-2]\n",
    "        assert dOutputZ_dW.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "\n",
    "        # dL_dOutputZ should be broadcasted up for each of the weights\n",
    "        dL_dOutputW = dL_dOutputZ * dOutputZ_dW\n",
    "        assert dL_dOutputW.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "        dL_dOutputW = np.sum(dL_dOutputW, axis=1, keepdims=True) / self.num_examples\n",
    "        assert dL_dOutputW.shape == (self.nodes_in_hidden_layer, 1)\n",
    "\n",
    "        # print(f\"dL_dOutputW = {dL_dOutputW}\")\n",
    "\n",
    "        dL_dOutputB = dL_dOutputZ # (dOutuptZ_dB == 1)\n",
    "        dL_dOutputB = np.sum(dL_dOutputB, axis=1, keepdims=True) / self.num_examples\n",
    "        assert dL_dOutputB.shape == (1, 1)\n",
    "        # print(f\"dL_dOutputB = {dL_dOutputB}\")\n",
    "\n",
    "        # -------------------- Done For Output Layer ---------------------------------------\n",
    "        # Now those are corrections made for this layer, but to propagate error backwards,\n",
    "        # we need to calculate error for the previous nodes as well.\n",
    "\n",
    "        dL_dA_l_1 = dL_dOutputZ * self.weights[-1] # dOutputZ_dA_l_1\n",
    "        assert dL_dA_l_1.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "\n",
    "        return (dL_dOutputW, dL_dOutputB, dL_dA_l_1)\n",
    "    \n",
    "\n",
    "    def hidden_loss(self, dLoss_dA_Lminus1: np.ndarray) -> (list[np.ndarray], list[np.ndarray]):\n",
    "        # dLoss_dA_curr_layer is done for all self.num_examples.\n",
    "        dLoss_dA_curr_layer = dLoss_dA_Lminus1\n",
    "        # print(f\"dLoss_dA_Lminus1 = {dLoss_dA_curr_layer}\")\n",
    "        assert dLoss_dA_curr_layer.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "        \n",
    "        dLoss_dWs = []\n",
    "        dLoss_dBs = []\n",
    "\n",
    "        for i in range(self.hidden_layers - 1, -1, -1):\n",
    "            dA_dZ = leaky_relu_derivative(self.Z[i])\n",
    "            assert dA_dZ.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "            \n",
    "            dLoss_dZ = dLoss_dA_curr_layer * dA_dZ\n",
    "            assert dLoss_dZ.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "\n",
    "            dZ_dW = self.activations[i]\n",
    "            # Really, it should be a 3D array, but it would actually just be a broadcast of each column, putting the examples in the 3rd dimension.\n",
    "            if i != 0:\n",
    "                assert dZ_dW.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "            else:\n",
    "                assert dZ_dW.shape == (self.input_size, self.num_examples)\n",
    "\n",
    "            # What's happening: dLoss_dZ's outer product with dZ_dW and then averaged over all training examples.\n",
    "            # dZ_dW is actually supposed to be a 2D matrix for one example.\n",
    "            # dLoss_dZ is supposed to be a 1D vector for one example.\n",
    "            # dLoss_dZ = [dLoss_dZ1, dLoss_dZ2, ....]\n",
    "            # dZ_dW    =[[dZ_dW00,   dZ_dW01,   dZ_dW02, ..]\n",
    "            #            [dZ_dW10,   dZ_dW11,   dZ_dW12, ..]\n",
    "            #               ...\n",
    "            #           ]\n",
    "            # But if you think about it, it's actually just:\n",
    "            # dZ_dW    =[[A(L-2, 0), A(L-2, 0), A(L-2, 0), ..]\n",
    "            #            [A(L-2, 1), A(L-2, 1), A(L-2, 1), ..]\n",
    "            #            ...\n",
    "            #           ]\n",
    "            # So really, for one example, it's just a 1D vector broadcasted into 2D.                    VVVV 1D part             , 1D part for all examples\n",
    "            # Hence our dZ_dW is technically having all information needed since its dimensions are (self.num_nodes_in_each_layer, self.num_examples).\n",
    "            # So, what we really want dLoss_dW to be is outer product of dLoss_dZ (1D form) and dZ_dW (1D form).\n",
    "            # and then since we have training examples, we average it over all training examples.\n",
    "            # Finally, the resulting weight differential should be arranged as (prev_layer X current_layer),\n",
    "            # and the resulting weight differential out of dL/dZ @ dZ/dW is actually (current_layer X prev_layer), \n",
    "            # so instead we do dZ/dW @ dL/dZ.\n",
    "            # This is what the simple dot product expression is doing. Magic to me how it all worked out almost coincidentally so well.\n",
    "            dLoss_dW = (dZ_dW.dot(dLoss_dZ.T)) / self.num_examples\n",
    "            if i != 0:\n",
    "                assert dLoss_dW.shape == (self.nodes_in_hidden_layer, self.nodes_in_hidden_layer)\n",
    "            else:\n",
    "                assert dLoss_dW.shape == (self.input_size, self.nodes_in_hidden_layer)\n",
    "            \n",
    "            dLoss_dWs.append(dLoss_dW)\n",
    "\n",
    "            dLoss_dB = np.sum(dLoss_dZ, axis=1, keepdims=True) / self.num_examples\n",
    "            assert dLoss_dB.shape == (self.nodes_in_hidden_layer, 1)\n",
    "\n",
    "            dLoss_dBs.append(dLoss_dB)\n",
    "\n",
    "            dLoss_dA_curr_layer = self.weights[i].dot(dLoss_dZ)\n",
    "            \n",
    "            # print(f\"dLoss_dA_curr_layer = {dLoss_dA_curr_layer}\")\n",
    "            \n",
    "            if i != 0:\n",
    "                assert dLoss_dA_curr_layer.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "            else:\n",
    "                assert dLoss_dA_curr_layer.shape == (self.input_size, self.num_examples)\n",
    "\n",
    "        return (list(reversed(dLoss_dWs)), list(reversed(dLoss_dBs)))\n",
    "            \n",
    "\n",
    "def sigmoid(x):\n",
    "    # Check if any element in the matrix is > 100_000\n",
    "    if np.any(x > 100_000):\n",
    "        print(x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(g_x):\n",
    "    return g_x * (1 - g_x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x < 0, 0, 1)\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return np.maximum(0.01*x, x)\n",
    "\n",
    "def leaky_relu_derivative(x: np.ndarray):\n",
    "    return np.where(x < 0, 0.01, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50877/1037052576.py:6: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  ys = np.array([float(int(x[1] % x[0] == 0)) for x in xs.T]).reshape(1, examples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 5 ... 8 8 3]\n",
      " [4 3 4 ... 1 3 0]] [[1. 0. 0. ... 0. 0. 1.]]\n",
      "Output = [[7.98774996e-43 4.59377566e-07 5.96420167e-42 ... 2.18384004e-45\n",
      "  5.27119652e-28 4.35292980e-34]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.57445\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.05471200e-42 4.85575242e-07 7.88885288e-42 ... 2.92671675e-45\n",
      "  6.36589242e-28 5.50174363e-34]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.57445\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.42111388e-42 5.15318648e-07 1.06491774e-41 ... 4.00681155e-45\n",
      "  7.79404951e-28 7.07320579e-34]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.57445\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.96035901e-42 5.49423557e-07 1.47195045e-41 ... 5.62313888e-45\n",
      "  9.69585650e-28 9.27532606e-34]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.57445\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[2.78011317e-42 5.88985926e-07 2.09200918e-41 ... 8.12506201e-45\n",
      "  1.22900097e-27 1.24497302e-33]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.57445\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[4.07521834e-42 6.35516526e-07 3.07383852e-41 ... 1.21565650e-44\n",
      "  1.59310927e-27 1.71821414e-33]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.57445\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[6.21885795e-42 6.86660411e-07 4.70295596e-41 ... 1.89762300e-44\n",
      "  2.12211982e-27 2.45298669e-33]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.57445\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[9.97668073e-42 7.45024511e-07 7.56654108e-41 ... 3.12247504e-44\n",
      "  2.92412152e-27 3.65241502e-33]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.57445\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.70604842e-41 8.13291504e-07 1.29807563e-40 ... 5.49566061e-44\n",
      "  4.20714503e-27 5.73830101e-33]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.57445\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[3.17503293e-41 8.99909086e-07 2.42456991e-40 ... 1.05751515e-43\n",
      "  6.40978325e-27 9.68007741e-33]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.57445\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[6.64551875e-41 1.01456102e-06 5.09584201e-40 ... 2.30330988e-43\n",
      "  1.05731174e-26 1.80234963e-32]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.57445\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.65589767e-40 1.17586965e-06 1.27581845e-39 ... 6.02940602e-43\n",
      "  1.96218269e-26 3.88474339e-32]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.57445\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[5.49377661e-40 1.42564499e-06 4.25559615e-39 ... 2.13475590e-42\n",
      "  4.41808325e-26 1.06442494e-31]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.57445\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[3.18598812e-39 1.88552737e-06 2.48025042e-38 ... 1.36261988e-41\n",
      "  1.44970936e-25 4.65241025e-31]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.57445\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[8.94726675e-38 3.35806707e-06 6.88808786e-37 ... 4.60562215e-40\n",
      "  1.37330345e-24 7.52705781e-30]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.57445\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[5.06521553e-32 2.77665996e-05 9.79350809e-32 ... 2.17074293e-34\n",
      "  1.06762381e-20 7.76834401e-26]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.605\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[5.10158980e-28 2.73152225e-04 4.89875013e-28 ... 4.80614321e-30\n",
      "  5.27587827e-18 1.08734422e-22]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.68\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.24322004e-26 5.10282672e-04 9.04370747e-27 ... 1.35407644e-28\n",
      "  3.40273149e-17 1.18474283e-21]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.68\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.42920166e-24 1.43032252e-03 1.18334267e-24 ... 2.07194399e-26\n",
      "  7.60617877e-16 6.39614104e-20]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.68\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[6.45140571e-20 1.50406349e-02 8.86800907e-20 ... 1.50259302e-21\n",
      "  9.62974951e-13 6.20527047e-16]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.68\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.70740973e-12 3.84919949e-01 2.67328382e-12 ... 8.84152529e-14\n",
      "  5.81459982e-08 7.96713329e-10]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.68975\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.04074091 0.96057374 0.11723504 ... 0.03807394 0.27137294 0.25977419]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.78385\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.06610339 0.94968315 0.12034199 ... 0.07210025 0.26605579 0.24626915]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.7833\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.08436536 0.9487898  0.03472681 ... 0.09354883 0.2799764  0.14803192]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.774\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.09678722 0.9472009  0.04359566 ... 0.10588291 0.29610246 0.16130632]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.774\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.12560039 0.94100026 0.08466895 ... 0.13023238 0.34848033 0.22801152]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.79365\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.12498546 0.93949766 0.18293765 ... 0.13260729 0.33555646 0.2462862 ]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.7835\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.14141598 0.93802109 0.08541818 ... 0.15039268 0.34521845 0.20947696]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.774\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.15527491 0.93431474 0.12383869 ... 0.1579466  0.37814177 0.24960183]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.78435\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.160755   0.93114493 0.13820527 ... 0.16406251 0.39011348 0.25597282]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.78435\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.17374371 0.92553386 0.16201777 ... 0.17275086 0.41346164 0.27143682]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.78435\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.16430072 0.92345282 0.17861027 ... 0.16657872 0.41257433 0.28098874]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.79385\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.15114765 0.91470155 0.18604571 ... 0.15455837 0.40023691 0.28086097]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.79385\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.14313234 0.91023181 0.1993238  ... 0.14697163 0.39396223 0.28687572]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.7835\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.12044999 0.90872165 0.27286295 ... 0.12812776 0.35521665 0.24554148]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8021\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.12814173 0.90835971 0.29517221 ... 0.13834191 0.36388834 0.25806251]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.80255\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.1177584  0.90733107 0.27552615 ... 0.12868524 0.34652906 0.24112646]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8113\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.11656508 0.90444022 0.27780166 ... 0.12801355 0.34255683 0.2396251 ]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8113\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.11357525 0.90181404 0.27207996 ... 0.12534488 0.33356874 0.232368  ]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8113\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.11391959 0.8987566  0.26668691 ... 0.12643473 0.32936744 0.23015564]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.81975\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.11292636 0.89430835 0.26921347 ... 0.12630515 0.32393069 0.22591415]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.84065\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.11086492 0.89067617 0.26585724 ... 0.12455798 0.31771841 0.22060603]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.84065\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.12379271 0.88987085 0.30431419 ... 0.13854789 0.33254047 0.23669654]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.84195\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.11058855 0.88623015 0.26449269 ... 0.12424198 0.31139474 0.21475763]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.85115\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.12035824 0.88275869 0.29784343 ... 0.13498784 0.3232679  0.22766346]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8607\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.10959937 0.87824587 0.24882925 ... 0.12605048 0.30407312 0.20794312]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.85115\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.10999897 0.87828059 0.2543085  ... 0.12705465 0.30131873 0.20507025]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8705\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.12943623 0.88066166 0.28742066 ... 0.15131265 0.32466159 0.2284663 ]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86025\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.10274369 0.87622158 0.21648094 ... 0.12012164 0.28344643 0.18566865]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8705\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.1039873  0.87683486 0.21572109 ... 0.12125054 0.28127705 0.18191615]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8802\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.13071412 0.87614059 0.25290092 ... 0.15095471 0.3134266  0.21312455]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86025\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.10201637 0.87269014 0.18535008 ... 0.12185298 0.26957599 0.16747984]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8802\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.10089063 0.86925377 0.16702606 ... 0.12205896 0.26513878 0.16215105]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8693\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.13713759 0.87297013 0.2054279  ... 0.16961109 0.29646472 0.18581888]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86025\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.11644976 0.86820909 0.14331833 ... 0.14723319 0.25883962 0.14604103]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8693\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.14061726 0.87077413 0.17069659 ... 0.18057672 0.28485608 0.16494034]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86025\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.14016402 0.8687991  0.15310428 ... 0.18042683 0.2819944  0.15928326]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86025\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.13737708 0.86658113 0.13466339 ... 0.17718874 0.2776268  0.15187386]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8704\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.15326673 0.86675655 0.1366282  ... 0.19823375 0.29449155 0.16133557]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86025\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.14349358 0.86256434 0.11291779 ... 0.18495787 0.28356115 0.1480537 ]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8704\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.15003669 0.86100507 0.10722627 ... 0.19348998 0.29062066 0.14886759]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86025\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.15222889 0.85883698 0.09788547 ... 0.19603821 0.29454877 0.14711771]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86025\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.15591055 0.8571155  0.0896928  ... 0.200284   0.29979948 0.14683784]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86025\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.15387502 0.85524097 0.07967269 ... 0.19881365 0.29976971 0.14251238]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86025\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.15825517 0.85416536 0.07411499 ... 0.20626877 0.30708733 0.14398329]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86025\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.17045189 0.85437799 0.07251652 ... 0.22261197 0.32212236 0.151233  ]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8493\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.18131107 0.85487397 0.06866566 ... 0.23649185 0.333572   0.15460166]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8493\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.16206124 0.84819486 0.05223972 ... 0.2070372  0.31031964 0.13049803]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86025\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.19481911 0.84789147 0.05414417 ... 0.24256038 0.33904255 0.14223143]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.85985\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.1695684  0.84199791 0.04124835 ... 0.20257467 0.31820386 0.12411462]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86025\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.20607089 0.84521298 0.04744595 ... 0.24200997 0.35535758 0.14215982]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.85985\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.22887246 0.84710879 0.0499688  ... 0.2624714  0.37970227 0.15213489]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.85985\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.17507588 0.84608004 0.03473524 ... 0.19800922 0.33607187 0.11891467]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8493\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.23623689 0.85112568 0.04674609 ... 0.26209447 0.3983355  0.15267434]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.85\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.21737035 0.84919107 0.03970724 ... 0.23601355 0.38828572 0.13942016]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.85985\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.18439711 0.84501062 0.03104953 ... 0.1976652  0.36453282 0.11939342]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.85985\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.29699574 0.85243052 0.05741144 ... 0.31202518 0.4762404  0.19050253]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.87075\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.16693252 0.84021498 0.02667994 ... 0.1770633  0.36839143 0.11224993]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8802\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.24977294 0.85420683 0.03672704 ... 0.27567306 0.45054875 0.15025038]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.87075\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.21788243 0.85392475 0.03284571 ... 0.23412615 0.44026621 0.14047996]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8702\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.25311873 0.86204633 0.03433534 ... 0.28413219 0.47751119 0.15293578]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.87075\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.1860897  0.86438584 0.02485742 ... 0.20664423 0.43461969 0.12279469]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.85985\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.21111755 0.87902968 0.02684301 ... 0.24462891 0.47221787 0.13576825]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8702\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.2391764  0.88629014 0.02706392 ... 0.28447325 0.50501432 0.14417605]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.88035\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.18423223 0.89174417 0.02328363 ... 0.20778992 0.47088242 0.11979516]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8702\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.06920765 0.87234511 0.00895175 ... 0.06646823 0.33400357 0.06350611]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8699\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.21220635 0.90481668 0.02190651 ... 0.25718952 0.52916833 0.13255069]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8902\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.09504569 0.89466565 0.01914621 ... 0.09502242 0.3959811  0.06933859]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8693\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.22000123 0.91264937 0.02383136 ... 0.27282347 0.55825557 0.12754248]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8813\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.22947251 0.92557594 0.0238487  ... 0.31503875 0.58371845 0.13267284]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8923\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.19745029 0.92546365 0.02234469 ... 0.25459499 0.56791113 0.11647572]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8813\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.00354607 0.87971373 0.00740604 ... 0.00227311 0.09848297 0.00673596]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86805\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.16720638 0.93272066 0.01866175 ... 0.21848803 0.55371992 0.0894454 ]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8813\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.08400459 0.92844103 0.00865185 ... 0.09817086 0.47939733 0.06444949]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.87965\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.23307613 0.9438388  0.02314841 ... 0.31953224 0.66682294 0.13250388]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.9014\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.08178357 0.93834891 0.00891157 ... 0.1045396  0.51874555 0.06593926]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.88925\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.01614213 0.92707837 0.00843928 ... 0.01743164 0.25785887 0.01455312]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8697\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.2756324  0.95266513 0.02168373 ... 0.39484389 0.72396931 0.13632611]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.80275\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.19698159 0.95077679 0.01606801 ... 0.30466483 0.66590145 0.09061372]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.9222\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.35422455 0.95807427 0.02073422 ... 0.45614063 0.79990412 0.19154563]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.77175\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.30035472 0.95789472 0.01664892 ... 0.40540829 0.7746759  0.14439476]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.79195\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.80110487e-04 8.88642056e-01 8.51559214e-04 ... 9.40979669e-05\n",
      "  3.08321097e-02 5.95362133e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.85775\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[2.11484979e-04 8.96363240e-01 2.87135118e-03 ... 9.94655589e-05\n",
      "  3.74375415e-02 7.32551970e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.85775\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.22266484 0.96155233 0.01375208 ... 0.30894936 0.76690699 0.10491666]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.79195\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.09609835 0.95638474 0.00657201 ... 0.14974825 0.64867319 0.04824142]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.93085\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[4.17837003e-04 9.14969021e-01 2.90490658e-03 ... 2.46972824e-04\n",
      "  6.07437540e-02 9.11301209e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86805\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.2848023  0.96590368 0.01122031 ... 0.39078011 0.80760969 0.09977945]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.79195\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.02784324 0.95366222 0.00627275 ... 0.03695515 0.4451816  0.01222918]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.87965\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.11622386 0.96218152 0.00537991 ... 0.16882246 0.71525578 0.0458461 ]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.92165\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[7.89426140e-04 9.31644424e-01 2.87102303e-03 ... 4.67558862e-04\n",
      "  1.09119210e-01 1.25133794e-03]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86805\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[2.19241749e-04 9.21873232e-01 2.31526506e-03 ... 9.88539905e-05\n",
      "  5.48545580e-02 4.40475289e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86805\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.21041063 0.96942073 0.00740869 ... 0.30555483 0.82309002 0.06637139]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8007\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.07339051 0.96528721 0.00410452 ... 0.09561635 0.70190881 0.02801822]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.93085\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.00950171 0.95703452 0.004861   ... 0.01063463 0.3806354  0.0046254 ]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8693\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.05295994 0.96655293 0.00342184 ... 0.06957014 0.68566193 0.02033499]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.9301\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.00397402 0.95449503 0.00115875 ... 0.00396906 0.33620475 0.00337276]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8685\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.08434503 0.9705113  0.00364058 ... 0.13217261 0.76902538 0.02709939]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.93085\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.31833691e-03 9.48680991e-01 1.21420399e-03 ... 8.64298618e-04\n",
      "  2.19999979e-01 1.26326178e-03]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86805\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.0060261  0.96034295 0.00132294 ... 0.00560647 0.44792186 0.00398862]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.88005\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.19465294 0.9735599  0.00471468 ... 0.30655851 0.87488271 0.04371045]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.7822\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.00803494 0.96383604 0.0016321  ... 0.00893927 0.50042075 0.00378311]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.88925\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.18157434 0.97406981 0.00509007 ... 0.26924858 0.8738042  0.03160681]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.79095\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.00745123 0.96374613 0.00174011 ... 0.00794066 0.5059943  0.00284738]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8874\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.49644382e-03 9.58581672e-01 9.32988648e-04 ... 1.51076354e-03\n",
      "  2.86570875e-01 8.28910794e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86785\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.05295059 0.97049405 0.00332983 ... 0.07711392 0.77273476 0.00851485]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.93085\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.15584188 0.97229437 0.00452023 ... 0.23187361 0.87843925 0.01777701]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.81095\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.02716587 0.96766883 0.00256598 ... 0.03263061 0.71695517 0.00439275]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.91995\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.00526575 0.96380453 0.00184988 ... 0.0052315  0.51611774 0.00134746]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8967\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[7.55293384e-04 9.57316156e-01 1.20223005e-03 ... 5.21514053e-04\n",
      "  2.74604189e-01 4.11096363e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.87745\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[2.01810533e-03 9.59721058e-01 1.64414520e-03 ... 1.97706381e-03\n",
      "  4.19974920e-01 6.51605245e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8871\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[3.06008911e-04 9.53178173e-01 9.77756537e-04 ... 2.92418950e-04\n",
      "  1.90128849e-01 2.62229909e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.87745\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.00876967 0.9619438  0.00188596 ... 0.01264801 0.65959452 0.00151735]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.93965\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.03164235 0.96522109 0.0026261  ... 0.05678265 0.81035075 0.00395366]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.92165\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.10501055e-03 9.53336521e-01 9.80042549e-04 ... 1.18953468e-03\n",
      "  3.84331256e-01 3.02327063e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.86785\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[0.00966041 0.96371923 0.0021602  ... 0.01524168 0.69871414 0.00106457]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.9598\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[2.70518447e-03 9.61201582e-01 1.04630235e-03 ... 4.79989548e-03\n",
      "  5.17332220e-01 2.98625494e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8967\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[6.94642630e-03 9.66424591e-01 1.79257058e-03 ... 1.45280779e-02\n",
      "  6.75057665e-01 6.05218334e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.9598\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[6.15200345e-04 9.60254616e-01 7.80106253e-04 ... 1.28860784e-03\n",
      "  3.35449328e-01 1.13688670e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8782\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[8.00896925e-04 9.64167069e-01 8.38076656e-04 ... 2.17197098e-03\n",
      "  3.99001176e-01 1.00941234e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8782\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.53475693e-02 9.73419474e-01 2.18082175e-03 ... 6.94693182e-02\n",
      "  7.92430672e-01 7.39215684e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.94945\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[4.16448817e-04 9.62164249e-01 6.47103876e-04 ... 1.19702721e-03\n",
      "  3.57433886e-01 5.56057522e-05]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8878\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.03755426e-03 9.66559097e-01 8.96550661e-04 ... 4.17931106e-03\n",
      "  5.03178448e-01 9.56286506e-05]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8878\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.53360742e-04 9.57572694e-01 4.77685376e-04 ... 4.51504594e-04\n",
      "  2.55296889e-01 3.12434415e-05]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.87745\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[9.12192108e-03 9.72266817e-01 1.79997725e-03 ... 4.71075272e-02\n",
      "  7.52973492e-01 2.46604778e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.9598\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.19478387e-03 9.67514138e-01 9.32090680e-04 ... 5.46443051e-03\n",
      "  5.91107993e-01 8.30973401e-05]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.91995\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[5.68692603e-03 9.71893674e-01 1.29826851e-03 ... 2.95005583e-02\n",
      "  7.70025799e-01 1.78638140e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.9598\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[6.76289927e-04 9.66706755e-01 7.89611267e-04 ... 3.10484719e-03\n",
      "  5.27785936e-01 5.05758124e-05]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.8878\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[3.41323145e-03 9.70860053e-01 1.16195467e-03 ... 1.83121649e-02\n",
      "  7.49224262e-01 1.01001451e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.9598\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.59015104e-02 9.73953490e-01 1.74573752e-03 ... 9.27846088e-02\n",
      "  8.68075833e-01 2.53120416e-04]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.9281\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "Output = [[1.39898946e-03 9.68762798e-01 7.09770299e-04 ... 7.26478564e-03\n",
      "  6.73435528e-01 5.06172387e-05]]\n",
      "Expected = [[0. 1. 0. ... 0. 1. 0.]]\n",
      "Accuracy = 0.9583\n",
      "Number of one classes = 8511.0, total examples = 20000\n",
      "[array([[-0.45713393,  0.47493747, -0.51101279, -0.65004565,  0.67791991,\n",
      "        -0.24342287, -0.81649996, -0.89596624, -0.81169986,  1.52369959],\n",
      "       [ 0.36996371,  0.58016461, -1.63268302,  0.26095083, -0.3612266 ,\n",
      "         1.22386631, -0.69516111,  0.19077894,  0.80866371,  0.77548129]]), array([[-1.08981228, -0.49078964, -0.1221895 ,  0.77223339,  1.08771206,\n",
      "        -0.73350387, -0.69505425,  0.51787471, -2.17717395, -0.2372903 ],\n",
      "       [ 0.45890302, -1.25542664, -0.43267052,  0.89314432,  0.77164616,\n",
      "        -0.09882512, -1.11254139,  0.69344743, -0.00242172, -0.73763289],\n",
      "       [-0.05691489,  0.98498547,  1.06484961, -0.05851201,  0.03634425,\n",
      "        -1.96163158,  0.33431944,  0.0903161 ,  0.20892812,  0.34397169],\n",
      "       [-0.43266715,  0.52240646,  0.3736445 ,  0.40152201, -0.60511743,\n",
      "        -0.32319967,  0.13255238,  1.02227926,  0.13563141, -0.00686118],\n",
      "       [ 0.56508334, -0.54002113,  0.55488917, -0.03463493,  1.36026903,\n",
      "         1.50537794,  0.40327591, -0.0442802 ,  0.39011881, -0.27028936],\n",
      "       [ 0.16419424, -0.29367211,  0.08954465,  0.32336752, -0.29404585,\n",
      "        -0.07447494,  0.25208826, -0.3849924 ,  0.54345269, -1.77728603],\n",
      "       [ 0.45482412,  0.57207818, -0.77304172,  0.16346865, -0.05596091,\n",
      "        -0.18711677, -0.03033842, -0.39465738, -0.72836791, -0.6001372 ],\n",
      "       [-0.38216069, -0.37936179,  0.51751056,  0.9732871 , -0.8028646 ,\n",
      "         0.68705747, -0.71258872,  0.60348274, -0.17974667, -0.36745661],\n",
      "       [ 0.28891316, -0.89445029,  0.79783791, -0.09716098, -0.97745215,\n",
      "         1.55040269,  1.30481507,  0.6229757 ,  0.62796814, -0.22422674],\n",
      "       [-0.6832315 , -1.10647498, -0.31630179,  0.56329562,  0.46757353,\n",
      "         0.405072  , -0.36803633, -1.88420075, -0.52984877, -0.66238284]]), array([[-2.73292202e-01, -4.58041490e-01,  1.04670740e+00,\n",
      "        -6.04092295e-01, -1.61601154e-02,  9.64986959e-01,\n",
      "        -2.38449405e+00, -1.42584773e+00, -1.04239627e+00,\n",
      "        -9.46722859e-01],\n",
      "       [-9.92526714e-01,  5.51197052e-01,  1.87960746e-01,\n",
      "         3.00663127e-01, -1.48493076e-01,  4.57161851e-01,\n",
      "        -1.20987623e+00,  9.13972015e-04,  1.70888472e-01,\n",
      "         7.08563603e-01],\n",
      "       [-1.30196569e+00,  7.36060532e-01, -7.46469474e-01,\n",
      "        -2.29439287e-01, -2.93646636e-01, -4.15732825e-01,\n",
      "        -1.96712759e-01,  1.61385218e+00,  6.18860151e-01,\n",
      "        -1.89970783e-01],\n",
      "       [ 5.62005004e-01, -6.00132952e-01, -3.28512771e-01,\n",
      "        -7.35332786e-02, -1.29557713e-02,  1.11824911e-01,\n",
      "        -8.11038292e-01,  1.42171926e+00, -2.36761725e-01,\n",
      "        -1.46727588e-01],\n",
      "       [ 9.90555883e-01, -8.00246983e-01, -3.78260248e-01,\n",
      "         1.47018898e-02,  1.61202618e+00,  9.67095898e-01,\n",
      "         1.19308703e-01, -5.92445844e-01, -5.27514756e-02,\n",
      "        -3.97205002e-01],\n",
      "       [ 6.42023330e-01,  1.05902944e+00, -5.94314654e-01,\n",
      "        -9.29139014e-01,  3.77410431e-02,  3.91305201e-01,\n",
      "        -4.98014977e-01, -1.16562406e-01,  6.93362145e-01,\n",
      "         1.24312310e+00],\n",
      "       [ 2.17220138e-01, -3.18411288e-01, -4.01639960e-02,\n",
      "        -2.39052988e-01,  1.69982277e+00, -1.02190796e-01,\n",
      "         6.30744759e-01,  8.82234735e-02,  5.88387497e-01,\n",
      "         1.39682766e+00],\n",
      "       [-9.72860823e-02,  5.38098727e-01, -3.19337500e-01,\n",
      "        -5.64592378e-02, -3.37323121e-01, -2.34440740e-02,\n",
      "         8.52912289e-01, -5.66953616e-01,  1.24631025e+00,\n",
      "         7.41670417e-01],\n",
      "       [ 1.75965191e+00,  7.34794168e-02, -8.56468304e-01,\n",
      "         3.79660077e-01, -7.38759829e-01,  3.18894612e-01,\n",
      "        -5.66765047e-01,  1.22723764e+00, -4.28985759e-01,\n",
      "         6.28088702e-01],\n",
      "       [-7.33668162e-01,  7.81148567e-01,  8.62376890e-01,\n",
      "        -3.16724759e-01,  1.19016965e+00,  3.80222320e-01,\n",
      "         1.86503496e-01,  2.03383611e-01,  3.02145953e-01,\n",
      "         1.14682919e+00]]), array([[-0.29068549, -1.29580688,  0.3821186 ,  0.55713844, -0.99366459,\n",
      "        -0.28095189, -0.26381324, -0.49264731,  0.15291731,  0.43102313],\n",
      "       [-0.37866528,  1.4884148 , -0.83805149, -0.79811581, -0.92173902,\n",
      "        -0.75962653, -1.50907892, -0.17771159,  0.48009759, -0.28220175],\n",
      "       [ 1.54568771, -0.07041483, -0.06527226, -1.09332168, -0.6670488 ,\n",
      "         0.28649232, -0.0041109 ,  0.92840278,  0.11669834,  0.10688962],\n",
      "       [-0.37135774,  0.67243802,  1.39101625,  0.34667495, -0.31769898,\n",
      "         1.52512714,  0.07747678,  0.66854846,  0.33744352,  0.49111099],\n",
      "       [-0.17168908, -0.28291067, -0.3665427 , -0.2080062 ,  0.17992163,\n",
      "        -0.10196411, -0.28660772, -0.73031772, -0.5217154 ,  0.02756352],\n",
      "       [ 0.59739759,  0.61171651, -0.2786203 ,  0.7163294 ,  0.12342682,\n",
      "         0.05471221, -1.18152112,  0.28950657, -0.82948917, -0.29746654],\n",
      "       [-0.62161474, -0.03563546,  0.481741  ,  1.08988034, -0.09158727,\n",
      "        -1.83872569, -1.04844168, -1.52761206, -0.49094964,  0.46643508],\n",
      "       [ 1.01501296,  0.16138204,  0.88377401,  1.14560124,  1.02938562,\n",
      "         0.97028126,  1.39596468,  0.26414586,  0.42680669,  1.11958732],\n",
      "       [ 0.43151932, -1.40045152,  0.72672875,  0.18558356,  0.30835198,\n",
      "         1.42694286,  0.58549252,  1.23787683, -0.28002098,  0.30984251],\n",
      "       [-0.53591667,  0.11432286, -1.60261431,  0.97462988, -0.42032909,\n",
      "         0.67931046, -0.43990432, -0.00572646, -0.76325038, -0.93482119]]), array([[ 0.92889103,  0.02259965, -0.3316795 ,  1.54395655,  1.02978949,\n",
      "         0.62612003,  0.27443222, -0.6979733 , -1.00726489,  0.50631535],\n",
      "       [-1.30465308, -0.24585765, -1.58044668,  0.24910928, -1.29374001,\n",
      "         0.48721859,  0.23345593, -0.21289642, -0.86779081,  0.15723837],\n",
      "       [ 0.96960036,  0.39843568,  0.14049554,  0.57325997, -0.69226239,\n",
      "         0.23350874, -0.81122941,  0.2105097 , -0.43509897,  0.76368021],\n",
      "       [-0.43398939, -1.14634795,  0.16143878, -0.62999898, -0.63152737,\n",
      "        -0.21749425,  0.67667285, -0.72508039,  1.34065991, -0.96553381],\n",
      "       [-0.36277837, -1.18358342, -0.76006776, -1.31450747,  1.55339596,\n",
      "        -0.32110552,  0.56170791, -0.3088657 , -0.06975388, -0.96513416],\n",
      "       [-0.0220978 ,  0.31673163,  0.42733578, -0.35280262,  0.64051321,\n",
      "        -0.38386647,  0.77637354,  0.46208527, -0.03978511,  1.0550424 ],\n",
      "       [ 1.16173233, -1.18523476, -0.50980065,  0.28505948,  0.8965864 ,\n",
      "        -0.58470535, -0.15365452,  0.26981191, -0.79442787,  0.71489353],\n",
      "       [ 0.22945144, -0.2729418 ,  0.65323082,  1.63124846,  0.48908563,\n",
      "         0.48299484,  0.67110724,  0.05330605, -0.64696058, -0.12966899],\n",
      "       [-0.6470769 ,  0.15597652, -0.64001936,  0.0764205 , -0.44308623,\n",
      "        -0.64932282, -0.59197247, -0.88032152,  0.65158222, -0.97511312],\n",
      "       [ 1.00612525, -0.21642644, -0.37316863,  0.22457233, -0.50135133,\n",
      "        -0.20456382,  0.39423269, -0.90576495, -0.95089157, -0.3398133 ]]), array([[ 1.12247519],\n",
      "       [ 0.87818719],\n",
      "       [-0.95126355],\n",
      "       [-0.4929893 ],\n",
      "       [ 0.34782329],\n",
      "       [ 0.90099464],\n",
      "       [-1.12289982],\n",
      "       [ 0.90975864],\n",
      "       [ 0.89911473],\n",
      "       [ 0.59158361]])]\n",
      "[array([[-0.19273029],\n",
      "       [-0.87433526],\n",
      "       [-0.50199285],\n",
      "       [ 0.81132382],\n",
      "       [ 0.13411409],\n",
      "       [ 2.19415852],\n",
      "       [-0.79983568],\n",
      "       [ 0.285572  ],\n",
      "       [-0.00709248],\n",
      "       [ 2.08173921]]), array([[ 0.03727115],\n",
      "       [ 2.45893677],\n",
      "       [ 0.69479148],\n",
      "       [-1.13053372],\n",
      "       [ 0.07888569],\n",
      "       [-1.55496552],\n",
      "       [-0.67122307],\n",
      "       [-0.25058223],\n",
      "       [-0.63314756],\n",
      "       [ 0.02914117]]), array([[-0.62492684],\n",
      "       [-0.36990789],\n",
      "       [ 1.40887272],\n",
      "       [-1.64755595],\n",
      "       [ 1.68963017],\n",
      "       [ 0.29095905],\n",
      "       [ 0.44522419],\n",
      "       [-0.80559651],\n",
      "       [ 0.17544102],\n",
      "       [ 0.49550774]]), array([[ 0.75740636],\n",
      "       [ 0.80172617],\n",
      "       [ 1.28966581],\n",
      "       [ 1.22721413],\n",
      "       [ 0.1361911 ],\n",
      "       [-0.34022221],\n",
      "       [ 2.38926386],\n",
      "       [ 1.48415656],\n",
      "       [-0.34009577],\n",
      "       [-0.89505042]]), array([[ 0.41856761],\n",
      "       [-1.21647885],\n",
      "       [ 1.93122224],\n",
      "       [-0.10228602],\n",
      "       [ 0.19559761],\n",
      "       [ 2.70175628],\n",
      "       [-0.5523666 ],\n",
      "       [-0.24911646],\n",
      "       [ 0.18617255],\n",
      "       [-0.04152332]]), array([[0.2428934]])]\n"
     ]
    }
   ],
   "source": [
    "# Create a neural network to determine if first number is a factor of second or not.\n",
    "nn = BinaryNeuralNetwork(2, 5, 10)\n",
    "# Generate training data.\n",
    "examples = 100_000\n",
    "xs = np.random.randint(0., 10., (2, examples))\n",
    "ys = np.array([float(int(x[1] % x[0] == 0)) for x in xs.T]).reshape(1, examples)\n",
    "# Split into training and testing data as a fraction of examples\n",
    "train_xs = xs[:, :int(0.8 * examples)]\n",
    "train_ys = ys[:, :int(0.8 * examples)]\n",
    "print(train_xs, train_ys)\n",
    "test_xs = xs[:, int(0.8 * examples):]\n",
    "test_ys = ys[:, int(0.8 * examples):]\n",
    "\n",
    "for _ in range(150):\n",
    "    for i in range(0, train_xs.shape[1], 500):\n",
    "        nn.forward_propagate(train_xs[:, i:i+500])\n",
    "        nn.backpropagate(train_ys[:, i:i+500], 0.01)\n",
    "\n",
    "\n",
    "    nn.forward_propagate(test_xs)\n",
    "    print(f\"Output = {nn.output}\")\n",
    "    print(f\"Expected = {test_ys}\")\n",
    "    print(f\"Accuracy = {np.sum(np.round(nn.output) == test_ys) / test_ys.shape[1]}\")\n",
    "    print(f\"Number of one classes = {np.sum(test_ys)}, total examples = {test_ys.shape[1]}\")\n",
    "    # Test the neural network\n",
    "\n",
    "\n",
    "print(nn.weights)\n",
    "print(nn.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

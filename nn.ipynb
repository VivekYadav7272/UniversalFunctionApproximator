{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryNeuralNetwork:\n",
    "    def __init__(self, input_size: int, hidden_layers: int, nodes_in_each_layer: int):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.nodes_in_hidden_layer = nodes_in_each_layer\n",
    "\n",
    "        self.weights = [np.random.randn(input_size, nodes_in_each_layer) / np.sqrt(input_size)]\n",
    "        self.bias = [np.random.randn(nodes_in_each_layer, 1)]\n",
    "\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            self.weights.append(np.random.randn(nodes_in_each_layer, nodes_in_each_layer) / np.sqrt(input_size))\n",
    "            self.bias.append(np.random.randn(nodes_in_each_layer, 1))\n",
    "        \n",
    "        self.weights.append(np.random.randn(nodes_in_each_layer, 1) / np.sqrt(input_size))\n",
    "        self.bias.append(np.random.randn(1, 1))\n",
    "\n",
    "    \n",
    "    def forward_propagate(self, input_xs: np.ndarray):\n",
    "        self.num_examples = input_xs.shape[1]\n",
    "        self.activations = [input_xs]\n",
    "        self.Z = []\n",
    "        for i in range(0, self.hidden_layers + 1):\n",
    "            Z: np.ndarray = self.weights[i].T.dot(self.activations[i]) + self.bias[i]\n",
    "            if i != self.hidden_layers:\n",
    "                assert Z.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "            else:\n",
    "                assert Z.shape == (1, self.num_examples)\n",
    "\n",
    "            activation = leaky_relu if i != self.hidden_layers else sigmoid\n",
    "            self.activations.append(activation(Z))\n",
    "            self.Z.append(Z)\n",
    "\n",
    "        self.output = self.activations[-1]\n",
    "        assert self.output.shape == (1, self.num_examples)\n",
    "        # print(self.output)\n",
    "\n",
    "    \n",
    "    def backpropagate(self, inp_ys: np.ndarray, learning_rate: float):\n",
    "        (dL_dOutputW, dL_dOutputB, dL_dA_Lminus1) = self.output_loss(inp_ys)\n",
    "        (dWeights, dBiases) = self.hidden_loss(dL_dA_Lminus1)\n",
    "        # print(f\"dWeights = {dWeights}\\n\\ndBiases = {dBiases}\")\n",
    "\n",
    "        self.weights[-1] -= learning_rate * dL_dOutputW\n",
    "        self.bias[-1] -= learning_rate * dL_dOutputB\n",
    "\n",
    "\n",
    "        for i in range(self.hidden_layers):\n",
    "            # print(i)\n",
    "            self.weights[i] -= learning_rate * dWeights[i]\n",
    "            self.bias[i] -= learning_rate * dBiases[i]\n",
    "            \n",
    "    \n",
    "\n",
    "    def output_loss(self, inp_ys: np.ndarray) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "        # ------------------ For Output Layer, ---------------------------------------\n",
    "\n",
    "        # print(f\"inp_ys = {inp_ys.shape} && self.output == {self.output.shape}\")\n",
    "        assert inp_ys.shape == self.output.shape\n",
    "\n",
    "        dL_dOutputA = -(inp_ys - self.output)\n",
    "        assert dL_dOutputA.shape == (1, self.num_examples)\n",
    "        dA_dOutputZ = sigmoid_derivative(self.activations[-1])\n",
    "        assert dA_dOutputZ.shape == (1, self.num_examples)\n",
    "        \n",
    "        dL_dOutputZ = dL_dOutputA * dA_dOutputZ\n",
    "        assert dL_dOutputZ.shape == (1, self.num_examples)\n",
    "\n",
    "        dOutputZ_dW = self.activations[-2]\n",
    "        assert dOutputZ_dW.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "\n",
    "        # dL_dOutputZ should be broadcasted up for each of the weights\n",
    "        dL_dOutputW = dL_dOutputZ * dOutputZ_dW\n",
    "        assert dL_dOutputW.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "        dL_dOutputW = np.sum(dL_dOutputW, axis=1, keepdims=True) / self.num_examples\n",
    "        assert dL_dOutputW.shape == (self.nodes_in_hidden_layer, 1)\n",
    "\n",
    "        # print(f\"dL_dOutputW = {dL_dOutputW}\")\n",
    "\n",
    "        dL_dOutputB = dL_dOutputZ # (dOutuptZ_dB == 1)\n",
    "        dL_dOutputB = np.sum(dL_dOutputB, axis=1, keepdims=True) / self.num_examples\n",
    "        assert dL_dOutputB.shape == (1, 1)\n",
    "        # print(f\"dL_dOutputB = {dL_dOutputB}\")\n",
    "\n",
    "        # -------------------- Done For Output Layer ---------------------------------------\n",
    "        # Now those are corrections made for this layer, but to propagate error backwards,\n",
    "        # we need to calculate error for the previous nodes as well.\n",
    "\n",
    "        dL_dA_l_1 = dL_dOutputZ * self.weights[-1] # dOutputZ_dA_l_1\n",
    "        assert dL_dA_l_1.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "\n",
    "        return (dL_dOutputW, dL_dOutputB, dL_dA_l_1)\n",
    "    \n",
    "\n",
    "    def hidden_loss(self, dLoss_dA_Lminus1: np.ndarray) -> (list[np.ndarray], list[np.ndarray]):\n",
    "        # dLoss_dA_curr_layer is done for all self.num_examples.\n",
    "        dLoss_dA_curr_layer = dLoss_dA_Lminus1\n",
    "        # print(f\"dLoss_dA_Lminus1 = {dLoss_dA_curr_layer}\")\n",
    "        assert dLoss_dA_curr_layer.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "        \n",
    "        dLoss_dWs = []\n",
    "        dLoss_dBs = []\n",
    "\n",
    "        for i in range(self.hidden_layers - 1, -1, -1):\n",
    "            dA_dZ = leaky_relu_derivative(self.Z[i])\n",
    "            assert dA_dZ.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "            \n",
    "            dLoss_dZ = dLoss_dA_curr_layer * dA_dZ\n",
    "            assert dLoss_dZ.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "\n",
    "            dZ_dW = self.activations[i]\n",
    "            # Really, it should be a 3D array, but it would actually just be a broadcast of each column, putting the examples in the 3rd dimension.\n",
    "            if i != 0:\n",
    "                assert dZ_dW.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "            else:\n",
    "                assert dZ_dW.shape == (self.input_size, self.num_examples)\n",
    "\n",
    "            # What's happening: dLoss_dZ's outer product with dZ_dW and then averaged over all training examples.\n",
    "            # dZ_dW is actually supposed to be a 2D matrix for one example.\n",
    "            # dLoss_dZ is supposed to be a 1D vector for one example.\n",
    "            # dLoss_dZ = [dLoss_dZ1, dLoss_dZ2, ....]\n",
    "            # dZ_dW    =[[dZ_dW00,   dZ_dW01,   dZ_dW02, ..]\n",
    "            #            [dZ_dW10,   dZ_dW11,   dZ_dW12, ..]\n",
    "            #               ...\n",
    "            #           ]\n",
    "            # But if you think about it, it's actually just:\n",
    "            # dZ_dW    =[[A(L-2, 0), A(L-2, 0), A(L-2, 0), ..]\n",
    "            #            [A(L-2, 1), A(L-2, 1), A(L-2, 1), ..]\n",
    "            #            ...\n",
    "            #           ]\n",
    "            # So really, for one example, it's just a 1D vector broadcasted into 2D.                    VVVV 1D part             , 1D part for all examples\n",
    "            # Hence our dZ_dW is technically having all information needed since its dimensions are (self.num_nodes_in_each_layer, self.num_examples).\n",
    "            # So, what we really want dLoss_dW to be is outer product of dLoss_dZ (1D form) and dZ_dW (1D form).\n",
    "            # and then since we have training examples, we average it over all training examples.\n",
    "            # Finally, the resulting weight differential should be arranged as (prev_layer X current_layer),\n",
    "            # and the resulting weight differential out of dL/dZ @ dZ/dW is actually (current_layer X prev_layer), \n",
    "            # so instead we do dZ/dW @ dL/dZ.\n",
    "            # This is what the simple dot product expression is doing. Magic to me how it all worked out almost coincidentally so well.\n",
    "            dLoss_dW = (dZ_dW.dot(dLoss_dZ.T)) / self.num_examples\n",
    "            if i != 0:\n",
    "                assert dLoss_dW.shape == (self.nodes_in_hidden_layer, self.nodes_in_hidden_layer)\n",
    "            else:\n",
    "                assert dLoss_dW.shape == (self.input_size, self.nodes_in_hidden_layer)\n",
    "            \n",
    "            dLoss_dWs.append(dLoss_dW)\n",
    "\n",
    "            dLoss_dB = np.sum(dLoss_dZ, axis=1, keepdims=True) / self.num_examples\n",
    "            assert dLoss_dB.shape == (self.nodes_in_hidden_layer, 1)\n",
    "\n",
    "            dLoss_dBs.append(dLoss_dB)\n",
    "\n",
    "            dLoss_dA_curr_layer = self.weights[i].dot(dLoss_dZ)\n",
    "            \n",
    "            # print(f\"dLoss_dA_curr_layer = {dLoss_dA_curr_layer}\")\n",
    "            \n",
    "            if i != 0:\n",
    "                assert dLoss_dA_curr_layer.shape == (self.nodes_in_hidden_layer, self.num_examples)\n",
    "            else:\n",
    "                assert dLoss_dA_curr_layer.shape == (self.input_size, self.num_examples)\n",
    "\n",
    "        return (list(reversed(dLoss_dWs)), list(reversed(dLoss_dBs)))\n",
    "            \n",
    "\n",
    "def sigmoid(x):\n",
    "    # Check if any element in the matrix is > 100_000\n",
    "    if np.any(x > 100_000):\n",
    "        print(x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(g_x):\n",
    "    return g_x * (1 - g_x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x < 0, 0, 1)\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return np.maximum(0.01*x, x)\n",
    "\n",
    "def leaky_relu_derivative(x: np.ndarray):\n",
    "    return np.where(x < 0, 0.01, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50877/1037052576.py:6: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  ys = np.array([float(int(x[1] % x[0] == 0)) for x in xs.T]).reshape(1, examples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 7 6 ... 9 6 8]\n",
      " [7 0 3 ... 7 3 1]] [[1. 1. 0. ... 0. 0. 0.]]\n",
      "Output = [[5.57883615e-01 2.86416930e-06 3.75806748e-09 ... 2.98249890e-06\n",
      "  2.92965483e-01 7.14367774e-01]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.6786\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.90820449 0.00913548 0.00097145 ... 0.06725103 0.63125573 0.65938951]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.76865\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.8697474  0.01410454 0.00097572 ... 0.08808693 0.61829413 0.74404944]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.7969\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.86980815 0.0181357  0.00110872 ... 0.0880835  0.53691123 0.65891725]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.7885\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.87258192 0.01850617 0.00115683 ... 0.08371673 0.50875588 0.64225665]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.789\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.87499359 0.02263205 0.00138575 ... 0.08256292 0.49956817 0.62549502]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8088\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.8791     0.03168526 0.00162114 ... 0.07914102 0.48765337 0.59499056]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.81975\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.88433338 0.03486829 0.00189045 ... 0.08309771 0.47970659 0.58072253]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.82965\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.89152747 0.03505764 0.00199861 ... 0.08833825 0.47451919 0.57739484]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.84065\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.89838012 0.0331809  0.0019998  ... 0.09096507 0.46859439 0.57111167]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8504\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.90207444 0.04090892 0.00249216 ... 0.08801722 0.4594975  0.55973737]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8612\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.90594242 0.05762213 0.00390795 ... 0.09120257 0.4510574  0.54964153]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8513\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.91311373 0.05614889 0.00472746 ... 0.09347761 0.44752234 0.54185909]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.86045\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.92313561 0.05579383 0.00703711 ... 0.09276817 0.44427752 0.54062557]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.86045\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.9281109  0.05520682 0.00999832 ... 0.09506958 0.44786612 0.54728505]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.86045\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.93410122 0.05541519 0.0129767  ... 0.09999993 0.45113638 0.54909602]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8702\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.93945515 0.05538014 0.01921023 ... 0.10617976 0.45541926 0.55145453]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8702\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.94330436 0.05792815 0.033516   ... 0.11854989 0.46155873 0.55829952]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8691\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.94685595 0.05939695 0.03754877 ... 0.13177125 0.46120103 0.55741766]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8786\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.94950114 0.06112747 0.04467748 ... 0.14318879 0.45899264 0.5525858 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8691\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.95178526 0.05979588 0.04716783 ... 0.15295415 0.45709278 0.5507868 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8691\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.95430533 0.06193613 0.05406372 ... 0.16239902 0.45553575 0.54843205]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8691\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.95636591 0.06215249 0.05522857 ... 0.17078393 0.45525759 0.54816495]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8691\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.95878401 0.06234911 0.05769837 ... 0.17828543 0.45328342 0.54721336]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8691\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.9606341  0.06356535 0.06562316 ... 0.18335492 0.45139005 0.54649492]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8691\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.96408558 0.06348007 0.06386155 ... 0.19581302 0.45818189 0.55527863]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8786\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.96529061 0.06493964 0.07056425 ... 0.19894181 0.45537399 0.54832568]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.8786\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.96638785 0.06603114 0.080227   ... 0.20116866 0.45751035 0.54470071]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.96782095 0.06566715 0.08004135 ... 0.20028586 0.45684086 0.54016255]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.96876421 0.06768575 0.08288889 ... 0.19725845 0.46373783 0.54045391]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.97064997 0.07828486 0.12013887 ... 0.21974218 0.46751841 0.53192052]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.9723853  0.07968917 0.13750674 ... 0.23151836 0.47070777 0.52887525]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.97299597 0.08109538 0.14761041 ... 0.24471416 0.46407201 0.52309948]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.97356213 0.08367479 0.15669534 ... 0.25386817 0.45520186 0.51940464]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.97434325 0.08353791 0.16061864 ... 0.25626589 0.45114552 0.51563696]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.97510499 0.08209828 0.16163766 ... 0.25436181 0.4453605  0.50770944]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.97590869 0.08421031 0.16886396 ... 0.25971114 0.44409002 0.51089035]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.9767218  0.08393221 0.17205467 ... 0.2606961  0.45208519 0.51640978]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.9777682  0.08403487 0.17550573 ... 0.26174783 0.44531858 0.51335161]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.97817036 0.08089693 0.17033045 ... 0.25015708 0.44251907 0.50649044]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.97867163 0.08426035 0.17950129 ... 0.25641109 0.44630651 0.5139996 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.97883083 0.08445479 0.18360915 ... 0.25807385 0.44701196 0.51669074]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.97918866 0.08448813 0.18729074 ... 0.26046106 0.44954717 0.51830503]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.97957736 0.08532611 0.19218104 ... 0.26412028 0.45198329 0.52128152]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.97988808 0.08350013 0.19129619 ... 0.26102615 0.44976047 0.51757182]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98004857 0.08262916 0.1917929  ... 0.25980171 0.44765224 0.51556792]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.9801584  0.08218866 0.19347016 ... 0.26053399 0.45532262 0.52128623]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98039624 0.08208727 0.19491344 ... 0.26020252 0.45235212 0.51897887]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98050087 0.0820531  0.19694892 ... 0.26148145 0.45098974 0.5181413 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98066533 0.08182922 0.19818497 ... 0.26179459 0.45203752 0.51907978]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.9808751  0.08153948 0.199935   ... 0.26336705 0.45584264 0.52110845]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98084376 0.08016499 0.19805884 ... 0.25960442 0.45693823 0.52022285]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98115989 0.08020713 0.20020027 ... 0.26141936 0.46435907 0.52506058]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98145487 0.07944248 0.19971967 ... 0.25998889 0.45801815 0.52142908]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98174208 0.07988466 0.20230571 ... 0.26216013 0.4521803  0.51886051]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98213546 0.07837699 0.20027823 ... 0.25884318 0.46214278 0.52520007]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98229817 0.07902351 0.2032811  ... 0.2616627  0.45510284 0.52178872]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98243467 0.07770723 0.20142508 ... 0.25827721 0.46339    0.52543053]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.9824828  0.07844858 0.20418667 ... 0.26074063 0.46172024 0.52518841]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98261476 0.0787262  0.20620657 ... 0.26279717 0.45936314 0.52424168]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98273145 0.07677908 0.20277698 ... 0.2585911  0.46314696 0.52572654]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98284671 0.07697984 0.20513524 ... 0.26113759 0.4593935  0.52406178]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98311929 0.07736393 0.20824608 ... 0.26459949 0.4574031  0.52393307]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98352062 0.0774898  0.21189068 ... 0.26924661 0.45833579 0.52497814]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98386739 0.07574988 0.21135045 ... 0.26918451 0.45900053 0.52397465]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98430332 0.07122794 0.20226068 ... 0.25907579 0.46088051 0.52295003]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98500576 0.07125764 0.20417016 ... 0.26030444 0.46691629 0.52759226]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98500544 0.07106903 0.20555354 ... 0.26193861 0.46416116 0.52625242]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98556769 0.0726344  0.21101257 ... 0.26720319 0.46788975 0.52894867]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98549361 0.07246102 0.21199621 ... 0.26807567 0.46064319 0.525122  ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98603322 0.07152561 0.21090928 ... 0.26653573 0.4648597  0.52880853]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.9860913  0.07143099 0.21210832 ... 0.2678741  0.45387875 0.5223437 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98658901 0.07043457 0.21103631 ... 0.26641634 0.46167402 0.52818656]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98702823 0.0707389  0.21329471 ... 0.26849038 0.46119273 0.52771566]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98717972 0.0688371  0.20921757 ... 0.263557   0.45820484 0.52697949]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98756141 0.0701649  0.21392867 ... 0.26842066 0.45767889 0.52685058]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98756727 0.06994038 0.21504633 ... 0.27028286 0.45573098 0.52674916]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98792865 0.06698803 0.20766802 ... 0.26150175 0.45614966 0.52642062]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98814231 0.06758804 0.21034568 ... 0.26407975 0.45356468 0.5259363 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98826761 0.06854346 0.21367654 ... 0.26705314 0.4508068  0.52459592]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98845666 0.067864   0.21266422 ... 0.26588794 0.44895739 0.52512822]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98868212 0.06794853 0.21398364 ... 0.26680798 0.45127324 0.5277464 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98889117 0.06781548 0.21459559 ... 0.26747071 0.45175404 0.52869424]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98922068 0.06632213 0.21087333 ... 0.26213458 0.4516341  0.52758041]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98920096 0.06708923 0.2140505  ... 0.26562004 0.44479206 0.5254209 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98943882 0.06641199 0.21309478 ... 0.26476902 0.44894577 0.5290523 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98954734 0.06488362 0.20955016 ... 0.26056579 0.4422634  0.52186222]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.98978548 0.06652975 0.21511684 ... 0.26601603 0.44715314 0.52793987]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99012389 0.06497014 0.21166163 ... 0.26216099 0.4510598  0.53108519]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99030985 0.06483836 0.21175333 ... 0.26138086 0.45075889 0.53136296]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99048366 0.06411287 0.2107712  ... 0.26017441 0.44950116 0.53222271]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99047955 0.0652276  0.21508965 ... 0.26502448 0.44674562 0.53190508]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99079342 0.06372569 0.21142425 ... 0.26052236 0.44837625 0.53288872]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99081535 0.06329589 0.21113779 ... 0.25994471 0.44344403 0.52961702]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99108147 0.06200962 0.20790003 ... 0.25603921 0.4448863  0.53215778]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99137726 0.06316578 0.21253161 ... 0.26137502 0.45168072 0.53750236]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99154096 0.06304446 0.21272808 ... 0.26109682 0.44999972 0.53596974]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99138662 0.06349101 0.21499764 ... 0.26351988 0.44237595 0.53278138]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99156833 0.06231284 0.21202904 ... 0.25977955 0.44296643 0.53407603]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99169063 0.06324149 0.21568868 ... 0.26379344 0.44091763 0.53370967]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99186183 0.06184673 0.21184521 ... 0.25891166 0.44230948 0.53484776]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99199466 0.06138171 0.21140967 ... 0.25854955 0.44151522 0.53587466]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99207751 0.06247441 0.21549721 ... 0.26262456 0.44010658 0.5353223 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99224814 0.06119253 0.21197882 ... 0.25845523 0.43913089 0.5346797 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99233228 0.06207519 0.21565969 ... 0.26250704 0.4392171  0.53642825]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99245826 0.06189899 0.21561208 ... 0.26203544 0.43695549 0.53657464]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99262292 0.06038305 0.21161696 ... 0.25750445 0.43822949 0.53656411]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99277724 0.06020408 0.21135835 ... 0.25696848 0.43858902 0.5373973 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99288054 0.05988353 0.21106779 ... 0.25656753 0.43748943 0.53766914]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99299832 0.05931559 0.21035558 ... 0.25592952 0.43933655 0.53912655]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99303257 0.06048234 0.21464585 ... 0.260512   0.43299987 0.5359463 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99316939 0.06056321 0.21539355 ... 0.26066179 0.43509086 0.53985219]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99333688 0.05912068 0.21123834 ... 0.25604793 0.43484016 0.53911174]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99359716 0.05946893 0.21242663 ... 0.2567657  0.43669619 0.54177685]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.87905\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99353541 0.05971287 0.21383197 ... 0.25819047 0.43508608 0.54176769]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.9936175  0.059964   0.21528958 ... 0.25985604 0.43644026 0.54241153]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99377732 0.05740032 0.20784517 ... 0.25185667 0.43448857 0.54312608]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99384222 0.05945905 0.21420389 ... 0.25787244 0.43230041 0.54116163]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99389515 0.05955323 0.21524329 ... 0.25903116 0.42768559 0.53803948]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99404165 0.05722275 0.20797663 ... 0.25061883 0.4287814  0.54010034]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99411592 0.05921426 0.21475288 ... 0.2574361  0.4300662  0.5411818 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99417097 0.05931448 0.21584692 ... 0.25883788 0.42772119 0.54125614]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99431439 0.05678874 0.20813656 ... 0.25070082 0.43032805 0.54333499]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99435182 0.05899817 0.21560922 ... 0.25833912 0.42583754 0.54257942]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99444735 0.05899186 0.21593882 ... 0.25830163 0.42873316 0.54420503]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99450521 0.05890864 0.2158624  ... 0.25762026 0.42431958 0.54197681]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99462661 0.05725018 0.2117367  ... 0.2540947  0.42891114 0.54694341]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99467562 0.05941281 0.21869087 ... 0.26037156 0.42588047 0.54507977]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99479268 0.05740709 0.21230429 ... 0.25345638 0.42488962 0.54577998]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99490741 0.05797604 0.21459934 ... 0.25526663 0.42809862 0.54807663]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99497945 0.05812877 0.21543936 ... 0.25629585 0.42872141 0.54958652]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99506032 0.05763836 0.21422969 ... 0.25485035 0.42634614 0.54827744]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99510835 0.05892724 0.21860689 ... 0.25949867 0.4234706  0.54727036]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99524448 0.05817558 0.21635772 ... 0.2561603  0.42954889 0.55107043]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99537674 0.0579308  0.2156866  ... 0.25488226 0.4302065  0.55276304]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.9954645  0.05702169 0.21326368 ... 0.2528189  0.42843587 0.55224241]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.9955217  0.05699611 0.21342815 ... 0.25259154 0.42939953 0.55266713]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99558353 0.05663786 0.21298647 ... 0.25236937 0.42595316 0.55233108]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99564094 0.05660035 0.21322535 ... 0.25234806 0.42703165 0.55295389]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99568464 0.05756353 0.21712457 ... 0.25665402 0.42428392 0.55173646]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99569337 0.05807    0.21903485 ... 0.25860027 0.41747263 0.54922907]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99578135 0.05729934 0.21692695 ... 0.25603073 0.42054182 0.55106732]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.9958105  0.0579926  0.21992303 ... 0.25962657 0.41774047 0.55153756]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99591772 0.0550251  0.21004779 ... 0.24831252 0.41941824 0.5527033 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99594696 0.05712568 0.21727678 ... 0.25572631 0.4189155  0.55402781]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99600086 0.05641043 0.21603642 ... 0.25498793 0.41915075 0.55305027]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99604746 0.05720487 0.2193261  ... 0.25851558 0.42158192 0.55627046]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99611669 0.05583364 0.21483874 ... 0.25355456 0.42122851 0.55747768]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99617438 0.054236   0.20947374 ... 0.24741675 0.41824064 0.5549313 ]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "Output = [[0.99621457 0.0556107  0.21490422 ... 0.2534623  0.42289376 0.55835816]]\n",
      "Expected = [[1. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy = 0.88855\n",
      "Number of one classes = 8362.0, total examples = 20000\n",
      "[array([[-1.16715981, -0.63568816, -0.14260855,  0.37255353,  0.77450719,\n",
      "        -0.02687777,  0.27039133, -0.35702329, -0.85911   ,  1.29991975],\n",
      "       [-0.04599043,  0.09443962, -0.65334606,  0.25174082, -0.02635675,\n",
      "         0.42385809,  0.79899056, -0.18297714, -0.19882995,  0.43621555]]), array([[-0.67538955,  0.3690233 ,  0.03694738, -0.23747265, -0.2579912 ,\n",
      "        -1.57178383,  0.48682281, -0.72266442,  0.40285684,  0.84728144],\n",
      "       [ 0.06782069, -0.60324813,  0.68222923, -0.34948329,  0.23045035,\n",
      "         0.63773311,  0.97512362,  0.33236724, -0.72271847, -0.30509192],\n",
      "       [-0.94468556, -0.10049177, -0.84355542, -0.05068518, -0.3027562 ,\n",
      "        -0.78540322, -0.64722601, -0.09496078,  1.26276836,  0.08442241],\n",
      "       [-0.17561979,  1.0295058 , -0.56061333,  0.24211995,  0.36959988,\n",
      "        -0.13377143,  0.4095892 , -0.68491389, -0.093212  , -0.12960755],\n",
      "       [-0.81306739,  0.89877495,  0.73135267,  0.28444838, -1.43570162,\n",
      "        -1.21896028, -0.86766324,  0.58367162, -0.07793476, -0.89769552],\n",
      "       [ 0.55421821,  0.07886711, -0.51440149, -0.77202402, -0.8981157 ,\n",
      "         1.09433087,  1.03851884, -0.06911909,  0.76568558, -0.60986677],\n",
      "       [-0.37355166,  0.44437742,  0.57076854, -0.62954444,  0.53703785,\n",
      "        -0.44578155, -0.39890832, -1.60051409, -0.78601885,  0.08309392],\n",
      "       [ 0.22766106,  1.50163097, -0.21177691, -0.88635956, -0.19781732,\n",
      "         1.6098394 , -0.35496619, -0.00589285, -0.36944136, -1.14423011],\n",
      "       [-0.21563313, -1.07556926,  0.50132403, -0.93450887, -0.07977934,\n",
      "         1.24542894, -0.19061061, -0.50772222, -0.56706489,  0.75170925],\n",
      "       [ 0.46147578, -0.75708347, -0.88724819, -0.67055731,  0.38451104,\n",
      "         0.12879888, -0.3431944 ,  0.22595032,  0.36219836,  0.8039927 ]]), array([[ 0.3995359 , -0.18647895, -0.14894571,  1.25774792,  0.31950261,\n",
      "         0.30616489, -0.76813538, -1.09113054, -0.33926961,  0.86509931],\n",
      "       [ 0.62877437,  0.17079445,  0.03802537,  0.33446481, -1.36975382,\n",
      "         0.31890156, -0.09842474,  0.72171182,  0.06166137, -0.04015938],\n",
      "       [ 0.42954578,  0.32492724, -0.06824104,  0.2497785 ,  0.03290251,\n",
      "        -0.12598026,  0.15707438,  0.63301384, -1.05381797, -0.07510924],\n",
      "       [ 1.44839203,  0.51301844, -0.13241623,  0.65923384, -0.4971352 ,\n",
      "         0.23512776, -0.68488097,  0.22209051, -1.12463169, -0.37345569],\n",
      "       [-0.94427981,  0.23772933,  0.50498412, -0.56149315, -0.7934685 ,\n",
      "        -0.54563725,  0.16128136, -0.65733016, -0.34848749,  1.01137668],\n",
      "       [-1.20955043, -0.48237423, -0.77812007, -0.80468644,  0.60869262,\n",
      "         0.530005  ,  1.25704637, -0.37957434, -0.66284942,  0.80429598],\n",
      "       [-0.72907854, -0.04477474, -0.04754318,  0.26986841,  0.27991595,\n",
      "         0.03942878, -0.80137803, -0.74249153, -0.87132117,  0.51002606],\n",
      "       [-0.39689853, -0.01338416, -1.92010076, -0.91421425,  0.41129023,\n",
      "         1.04064478,  0.08765754, -0.27900795,  0.63985406,  1.17932501],\n",
      "       [-0.39182599, -0.55373388, -0.48983765, -0.41718958, -0.18004353,\n",
      "         0.06266736,  0.99999306, -0.38972832,  0.01607873, -0.23786192],\n",
      "       [ 1.54224101,  0.8454715 , -0.08194635,  0.29211068, -0.5267803 ,\n",
      "        -0.5973571 ,  0.15813233,  0.85720096, -0.13356651, -0.37127388]]), array([[ 3.82590031e-01, -9.74784715e-01,  3.88449451e-02,\n",
      "         1.33499616e+00,  4.05621649e-01, -4.32752292e-01,\n",
      "        -3.05377140e-02, -1.89786384e-01,  4.97704755e-01,\n",
      "         7.50601392e-02],\n",
      "       [ 7.45036208e-01, -6.36670464e-01,  9.92970935e-01,\n",
      "         3.70920993e-01, -1.39389260e-02, -1.18759515e+00,\n",
      "         1.36522122e+00, -3.80752006e-02,  5.50648291e-01,\n",
      "         1.91959167e+00],\n",
      "       [ 4.00895360e-01, -4.63257094e-01, -1.81852027e-01,\n",
      "         2.14391335e-01, -1.36473173e+00, -2.29138254e-02,\n",
      "        -2.10592137e-01,  1.46180247e-01,  5.40426427e-01,\n",
      "        -8.99886828e-02],\n",
      "       [ 7.09286106e-02, -3.53480955e-01, -8.94914740e-01,\n",
      "        -4.98755630e-01, -1.29837061e+00,  2.25842705e-01,\n",
      "        -1.08972367e+00, -9.62937869e-01, -2.61195339e-01,\n",
      "        -7.22796180e-04],\n",
      "       [ 4.12344396e-01, -2.25204410e-01, -1.50973529e+00,\n",
      "         3.18721531e-01, -1.15168177e+00, -2.14596066e-01,\n",
      "         1.52908416e+00,  8.87146023e-01, -2.26966872e-01,\n",
      "        -8.10992287e-01],\n",
      "       [ 7.40863125e-01,  5.65172929e-01,  7.29240654e-01,\n",
      "        -1.40891979e-02, -3.30844955e-01, -2.17133679e-01,\n",
      "        -4.34090087e-01,  5.21110096e-01, -1.02960610e+00,\n",
      "        -2.43623856e-01],\n",
      "       [ 3.23440731e-01, -2.80491655e-01, -5.99171415e-01,\n",
      "        -3.81963196e-01, -6.13051398e-01,  5.44894091e-01,\n",
      "        -4.82290675e-01,  8.96753816e-04, -7.74317115e-01,\n",
      "         9.32202741e-02],\n",
      "       [-2.79207364e-01,  1.16715789e+00, -1.07027842e+00,\n",
      "        -2.78732270e-01, -1.69796007e+00, -3.84761236e-01,\n",
      "        -9.96575813e-02,  1.18305153e+00, -3.20161530e-01,\n",
      "        -6.61491276e-01],\n",
      "       [ 5.11950213e-01,  6.10652207e-01,  9.58798359e-01,\n",
      "        -3.35819266e-01, -2.90008378e-01,  3.88297222e-01,\n",
      "         1.41477077e+00, -1.39605449e+00, -9.62564845e-01,\n",
      "        -2.48871758e-01],\n",
      "       [-8.65876757e-01, -1.00857036e+00, -5.65311909e-01,\n",
      "         7.05137893e-03,  6.96274502e-01,  5.82854706e-01,\n",
      "        -6.44701677e-01,  5.73709866e-01, -8.49807662e-02,\n",
      "         6.14214722e-01]]), array([[-0.28448024,  0.65244039, -0.33995056, -0.14309683, -0.62072391,\n",
      "        -0.40158775,  0.75386955,  0.7284075 ,  1.11790334,  0.0023872 ],\n",
      "       [-0.95057203,  0.13008069, -1.68988608,  0.67379321, -0.22148098,\n",
      "        -1.15049238,  0.42758615, -0.48049536,  0.02836968, -0.3786005 ],\n",
      "       [ 0.60227775,  0.06649057, -0.70598554, -0.13760288, -0.38381303,\n",
      "        -0.97027505,  0.29138948,  0.85650285,  0.75983578,  0.22601687],\n",
      "       [-0.03963458,  0.42031189,  0.67759069,  1.34706678, -1.73708384,\n",
      "         0.04716865, -0.09977804,  0.33581841, -0.29841093,  0.46820942],\n",
      "       [ 0.96411411,  0.42135604, -0.00470884, -0.0516617 ,  0.85789836,\n",
      "         1.30720693,  0.62048309,  0.22963348,  1.01530257,  0.35650557],\n",
      "       [ 0.50283654,  1.26857186,  0.588024  ,  0.37639989,  0.05115133,\n",
      "         0.0644041 ,  0.55565967, -0.17667817, -0.10465994,  0.13669184],\n",
      "       [-0.87631373, -1.83634432,  0.42892316, -0.19607072,  0.70466127,\n",
      "         1.32149891, -1.09845271, -0.00450952, -0.22953721,  0.4089266 ],\n",
      "       [ 0.28248548,  0.38133332, -0.5579352 , -1.26812088,  1.46246419,\n",
      "        -0.66371645, -0.37817426,  0.0386949 ,  0.69377937, -0.56865114],\n",
      "       [ 1.77871933,  0.92291276, -0.79338854, -0.08867177, -0.55574223,\n",
      "        -0.11908483,  0.63010843,  1.03597537, -1.09676507,  1.13086338],\n",
      "       [ 0.09169799, -0.70534066, -1.14115253, -0.03637152,  0.1406432 ,\n",
      "         0.02792635, -0.54519598,  0.81156724,  0.44972644,  0.69872588]]), array([[ 0.2917469 ],\n",
      "       [ 0.26948496],\n",
      "       [ 1.24696329],\n",
      "       [-0.14538747],\n",
      "       [ 0.82795837],\n",
      "       [ 0.9227857 ],\n",
      "       [-0.29879648],\n",
      "       [-1.54213598],\n",
      "       [-0.60933092],\n",
      "       [-0.48572382]])]\n",
      "[array([[ 1.20807546],\n",
      "       [ 1.61018546],\n",
      "       [ 0.20621222],\n",
      "       [-1.75452008],\n",
      "       [ 1.20503213],\n",
      "       [ 0.36591416],\n",
      "       [ 0.32421377],\n",
      "       [ 0.20599407],\n",
      "       [-0.33490861],\n",
      "       [ 0.80654484]]), array([[-0.55096876],\n",
      "       [ 0.37800144],\n",
      "       [-0.65546031],\n",
      "       [-0.95093669],\n",
      "       [-1.31935464],\n",
      "       [ 1.08718966],\n",
      "       [ 0.86854513],\n",
      "       [-0.56954101],\n",
      "       [ 1.15392781],\n",
      "       [-0.79716329]]), array([[ 0.00528262],\n",
      "       [-0.6741288 ],\n",
      "       [-1.07517697],\n",
      "       [-1.84128579],\n",
      "       [ 0.91622457],\n",
      "       [-1.63536871],\n",
      "       [ 0.11387174],\n",
      "       [ 0.31930963],\n",
      "       [ 1.86463928],\n",
      "       [ 1.11073475]]), array([[ 0.86643862],\n",
      "       [ 0.67542939],\n",
      "       [ 0.55624845],\n",
      "       [ 0.29582579],\n",
      "       [ 0.39997763],\n",
      "       [-0.30355152],\n",
      "       [ 0.42436429],\n",
      "       [-2.06552509],\n",
      "       [-0.45857832],\n",
      "       [-1.05314055]]), array([[ 0.62181215],\n",
      "       [-0.01252445],\n",
      "       [ 0.94214734],\n",
      "       [-0.22991136],\n",
      "       [ 0.42710141],\n",
      "       [ 2.10654384],\n",
      "       [ 0.71880579],\n",
      "       [-1.63389591],\n",
      "       [-0.32426613],\n",
      "       [ 1.45253271]]), array([[-1.18853141]])]\n"
     ]
    }
   ],
   "source": [
    "# Create a neural network to determine if first number is a factor of second or not.\n",
    "nn = BinaryNeuralNetwork(2, 5, 10)\n",
    "# Generate training data.\n",
    "examples = 100_000\n",
    "xs = np.random.randint(0., 10., (2, examples))\n",
    "ys = np.array([float(int(x[1] % x[0] == 0)) for x in xs.T]).reshape(1, examples)\n",
    "# Split into training and testing data as a fraction of examples\n",
    "train_xs = xs[:, :int(0.8 * examples)]\n",
    "train_ys = ys[:, :int(0.8 * examples)]\n",
    "print(train_xs, train_ys)\n",
    "test_xs = xs[:, int(0.8 * examples):]\n",
    "test_ys = ys[:, int(0.8 * examples):]\n",
    "\n",
    "for _ in range(150):\n",
    "    for i in range(0, train_xs.shape[1], 500):\n",
    "        nn.forward_propagate(train_xs[:, i:i+500])\n",
    "        nn.backpropagate(train_ys[:, i:i+500], 0.01)\n",
    "\n",
    "\n",
    "    nn.forward_propagate(test_xs)\n",
    "    print(f\"Output = {nn.output}\")\n",
    "    print(f\"Expected = {test_ys}\")\n",
    "    print(f\"Accuracy = {np.sum(np.round(nn.output) == test_ys) / test_ys.shape[1]}\")\n",
    "    print(f\"Number of one classes = {np.sum(test_ys)}, total examples = {test_ys.shape[1]}\")\n",
    "    # Test the neural network\n",
    "\n",
    "\n",
    "print(nn.weights)\n",
    "print(nn.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
